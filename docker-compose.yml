version: "3.7"

services:

  # Data Exploration and Computation


  #Jupyter notebook

  jupyter:
    container_name: "docker_data_science_stack_jupyter"
    restart: "always"
    image: jupyter/pyspark-notebook
    env_file:
      - ./config/jupyter.env
      - ./config/minio.env
      - ./config/shared_database.env
    volumes:
      - ./shared/notebooks/:/home/jovyan/work/notebooks
      - ./shared/dags/:/home/jovyan/work/dags
      - ./shared/spark/resources/data:/home/jovyan/work/data/
      - ./shared/spark/resources/jars:/home/jovyan/work/jars/
      - ./services/apistar/api/:/home/jovyan/work/api
    ports:
      - 8888:8888
      - "4040-4080:4040-4080"

  # Visualisation

  superset:
    container_name: "docker_docker_data_science_stack_superset"
    restart: "always"
    image: tylerfowler/superset
    depends_on:
      - postgres
    env_file:
      - ./config/superset_container.env
      - ./config/superset_database.env
    ports:
      - 8088:8088

  # Storage

  postgres:
    container_name: "docker_data_science_stack_postgres"
    restart: "always"
    image: postgres
    env_file:
      - ./config/postgres.env
      - ./config/superset_database.env
      - ./config/airflow_database.env
      - ./config/shared_database.env
    volumes:
      - postgres_volume:/var/lib/postgresql/data/
      - ./services/postgres/:/docker-entrypoint-initdb.d/
    ports:
      - 5432:5432

  pgadmin:
    container_name: "docker_data_science_stack_pgadmin"
    restart: "always"
    image: dpage/pgadmin4
    links:
      - postgres
    depends_on:
      - postgres
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: docker321
    volumes:
      - pgadmin:/root/.pgadmin
    ports:
      - 5050:80

  # Scheduler

  airflow:
    container_name: "docker_data_science_stack_airflow"
    restart: "always"
    image: puckel/docker-airflow
    depends_on:
      - postgres
    env_file:
      - ./config/airflow_container.env
      - ./config/minio.env
      - ./config/shared_database.env
    volumes:
      - ./shared/requirements.txt:/requirements.txt
      - ./shared/dags/:/usr/local/airflow/dags
    ports:
      - 8080:8080
    command: webserver

  # Model Utilities

  minio:
    container_name: "docker_data_science_stack_minio"
    restart: "always"
    image: minio/minio
    env_file:
      - ./config/minio.env
    volumes:
      - minio_volume:/data
    ports:
      - 9000:9000
      - 9010:9010
    command: server /data --console-address ":9010"

  # API

  apistar:
    container_name: "docker_data_science_stack_apistar"
    restart: "always"
    build: services/apistar
    env_file:
      - ./config/minio.env
    volumes:
      - ./services/apistar/api:/usr/src/app
    ports:
      - 8000:8000
    command: gunicorn app:app -b 0.0.0.0:8000

  # Docker jovyanistration

  portainer:
    container_name: "docker_data_science_stack_portainer"
    restart: "always"
    image: portainer/portainer
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_volume:/data
    ports:
      - 9090:9000
    command: -H unix:///var/run/docker.sock

  # Spark with 3 workers
  spark:
    image: bitnami/spark:latest
    user: root # Run container as root container: https://docs.bitnami.com/tutorials/work-with-non-root-containers/
    hostname: spark
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./shared/spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
      - ./shared/spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
    ports:
      - "8181:8080"
      - "7077:7077"

  spark-worker-1:
    image: bitnami/spark:latest
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./shared/spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
      - ./shared/spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

  spark-worker-2:
    image: bitnami/spark:latest
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./shared/spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
      - ./shared/spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

  spark-worker-3:
    image: bitnami/spark:latest
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./shared/spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
      - ./shared/spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)



# Volumes to persist data

volumes:
  postgres_volume:
  minio_volume:
  portainer_volume:
  pgadmin:
